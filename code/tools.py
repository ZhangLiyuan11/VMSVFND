import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from transformers import BertTokenizer, BertModel


class LayerNormalization(nn.Module):
    def __init__(self, d_hid, eps=1e-6):
        super(LayerNormalization, self).__init__()
        self.gamma = nn.Parameter(torch.ones(d_hid))
        self.beta = nn.Parameter(torch.zeros(d_hid))
        self.eps = eps

    def forward(self, z):
        mean = z.mean(dim=-1, keepdim=True, )
        std = z.std(dim=-1, keepdim=True, )
        ln_out = (z - mean) / (std + self.eps)
        ln_out = self.gamma * ln_out + self.beta

        return ln_out


def pretrain_bert_token():
    print('dataloader中加载token')
    tokenizer = BertTokenizer.from_pretrained("pretrain/roberta_wwm")
    return tokenizer


def pretrain_bert_models():
    # tokenizer = BertTokenizer.from_pretrained("pretrain/roberta_wwm")
    print('model中加载bert')
    model = BertModel.from_pretrained("pretrain/roberta_wwm").cuda()
    for param in model.parameters():
        param.requires_grad = False
    return model

class PoswiseFeedForwardNet(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PoswiseFeedForwardNet, self).__init__()
        self.relu = nn.ReLU()
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = LayerNormalization(d_model)

    def forward(self, inputs):
        # inputs: [b_size x len_q x d_model]
        residual = inputs
        output = self.relu(self.conv1(inputs.transpose(1, 2)))

        # outputs: [b_size x len_q x d_model]
        output = self.conv2(output).transpose(1, 2)
        output = self.dropout(output)

        return self.layer_norm(residual + output)


class FeedForward(nn.Module):
    def __init__(self, model_dimension, d_ff, dropout=0.1):
        super().__init__()
        self.ff1 = nn.Linear(model_dimension, d_ff)
        self.ff2 = nn.Linear(d_ff, model_dimension)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(model_dimension)

    def forward(self, representations_batch):
        return representations_batch + self.norm(self.ff2(self.dropout(F.relu(self.ff1(representations_batch)))))

